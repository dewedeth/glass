// THIS FILE WAS GENERATED BY GLASS -- DO NOT EDIT!

import { runGlass, useState } from '@glass-lang/glasslib'

export function getA1685490980324Prompt() {
  function getTestData() {
    return {}
  }

  const compile = async (opt: {}) => {
    const GLASS_STATE = {}

    // Welcome to Glass!
    // Glass provides a first-class developer experience for working with LLMs.
    // Learning Glass is as simple as playing with a few example Glass files.
    // Let's start with a simple interaction with gpt-3.5-turbo, the formal name for ChatGPT.
    // Now add some text below between <User> and </User>.
    // Then, run the `Glass: run file` command to generate a completion.
    // You can do this by opening up the VSCode command menu `command+shift+P`
    // and selecting the `Glass: run file` command.
    // This is equivalent to hitting the OpenAI API with two chat blocks:
    // {role: 'system', content: 'You are HaikuGPT. Always respond to the user in the form of a haiku.'}
    // {role: 'user', content: 'Hey, how are you?'}

    const GLASSVAR = {}
    const TEMPLATE = `// Welcome to Glass!

// Glass provides a first-class developer experience for working with LLMs.
// Learning Glass is as simple as playing with a few example Glass files.

// Let's start with a simple interaction with gpt-3.5-turbo, the formal name for ChatGPT.

<System>
You are HaikuGPT. Always respond to the user in the form of a haiku.
</System>

// Now add some text below between <User> and </User>.

<Chat model="gpt-3.5-turbo" />



// Then, run the \`Glass: run file\` command to generate a completion.
// You can do this by opening up the VSCode command menu \`command+shift+P\`
// and selecting the \`Glass: run file\` command.

// This is equivalent to hitting the OpenAI API with two chat blocks:
// {role: 'system', content: 'You are HaikuGPT. Always respond to the user in the form of a haiku.'}
// {role: 'user', content: 'Hey, how are you?'}`
    return {
      fileName: 'a.1685490980324.glass',
      model: 'gpt-3.5-turbo',
      interpolatedDoc: TEMPLATE,
      originalDoc:
        "// Welcome to Glass!\n\n// Glass provides a first-class developer experience for working with LLMs.\n// Learning Glass is as simple as playing with a few example Glass files.\n\n// Let's start with a simple interaction with gpt-3.5-turbo, the formal name for ChatGPT.\n\n<System>\nYou are HaikuGPT. Always respond to the user in the form of a haiku.\n</System>\n\n// Now add some text below between <User> and </User>.\n\n<Chat model=\"gpt-3.5-turbo\" />\n\n\n\n// Then, run the `Glass: run file` command to generate a completion.\n// You can do this by opening up the VSCode command menu `command+shift+P`\n// and selecting the `Glass: run file` command.\n\n// This is equivalent to hitting the OpenAI API with two chat blocks:\n// {role: 'system', content: 'You are HaikuGPT. Always respond to the user in the form of a haiku.'}\n// {role: 'user', content: 'Hey, how are you?'}",
      state: GLASS_STATE,
      interpolationArgs: opt.args || {},
      onResponse: undefined,
    }
  }

  return { getTestData, compile }
}

export const Glass = {
  a1685490980324Glass: getA1685490980324Prompt,
}

const { getTestData, compile } = getA1685490980324Prompt()

;(async function run() {
  const t = getTestData()
  const res: any[] = []
  if (Array.isArray(t)) {
    for (const args of t) {
      const c: any = await compile({ args: { ...args, ...({"input":"hi"}) } })
      res.push(c)
    }
  } else {
    const c: any = await compile({ args: { ...t, ...({"input":"hi"}) } })
    res.push(c)
  }
  const ret = await runGlass(res[0], { progress: (data: { nextDoc: string; nextInterpolatedDoc: string; rawResponse?: string }) => {
    // console.log('glass-progress: ' + JSON.stringify(data))
  } })
  console.log('glass-result: ' +  JSON.stringify(ret))
})()
